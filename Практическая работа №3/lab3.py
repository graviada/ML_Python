import numpy as np
import struct
from functools import reduce
from scipy.optimize import minimize
from matplotlib import pyplot as plt

# Вам предлагается обучить трехслойную нейронную сеть (входной, 1 скрытый, выходной слои) прямого распространения с
# сигмоидальными функциями активации на
# наборе данных MNIST - множестве изображений рукописных цифр и соответствующих метках 0-9.

# Ваша задача заполнить код в следующих функциях:
# logistic - вычисляет логистическую функцию от аргумента,
# compute_hypothesis - вычисляет выходы сети для заданного набора данных
# (на вход фактически подается сложная функция, представленная в виде нейросети, значение от входов нейросети
# распространяет на выходы - т. е. по сути осуществляет алгоритм прямого распространения сигнала),
# compute_cost - вычисляет функцию стоимости,
# compute_cost_grad - вычисляет градиент функции стоимости с помощью алгоритма обратного распространения ошибки.

# ____________NEURON WORK EXPLAINED____________
# Нейрону подаются на вход значения. Он их суммирует внутри себя, умножая на какие-то числа, а затем составляет
# ФУНКЦИЮ АКТИВАЦИИ от этой суммы. На входе - некоторый фиктивный признак с некоторым параметром theta_0 (весом).
# х_0 - первый вход - равен 1.
# Нейрон суммирует все theta_j, помноженные на x_j и берет от суммы функцию активации f.
# f - логистическая функция (сигмоидальная функция из 2-ой практики).
# Модель, которую задает 1 нейрон - логистическая модель.
# Чтобы обучить 1 нейрон, нам так же нужна еще функция стоимости.
# Принцип "один против всех" - когда один класс обозначается единицей, а остальные нулями при многоклассовой задаче.
# Унитарный вектор - вектор из одной единицы и нулей.
# ______________________________________________

# По ходу работы смотрите в консоль. Там будут выводиться результаты написанных вами функций и ожидаемые результаты.
# Так вы сможете проверить себя.

# Ниже заданы "константы", с которыми можно "поиграть" - поварьировать их и посмотреть, что будет.
HIDDEN_LAYER_SIZE = 36  # Количество нейронов в скрытом слое.

# Всего у вас есть 60 000 изображений, поэтому сумма следующих двух констант не должна быть больше 60 000.
SAMPLES_TO_TRAIN = 1500  # Количество примеров в обучающей выборке, на ней будет производить подбор параметров модели.
SAMPLES_TO_TEST = 100  # Количество примеров в тестовой выборке, на ней будет оцениваться качество модели.


def one_hot_encode(y):
    # y - вектор меток объектов из всей обучающей выборки
    # Метки классов должны быть преобразованы в унитарный вектор.
    # Каждому числу (классу) ставим в соотвествие его унитарный вектор.
    # Каждый нейрон на выходе соотвествует какому-либо классу. Мы подаем объект одного из классов на вход.
    # Чтобы понять, что нужно давать на выходе, мы должны исходные метки преобразовать в унитарный вектор.
    # Функция кодирует вектор чисел с метками классов в матрицу, каждая строка которой является унитарным кодом
    # соответствующей метки.

    return np.array(np.arange(0, 10, 1) == y, dtype=np.int32)


def get_random_matrix(l_in, l_out):
    # Генерация матрицы из theta.
    # Функция для генерации матрицы случайных параметров на основе равномерного распределения.
    # Theta задаются случайно, так как нейронная сеть - это не линейная модель, ее функция активации не является
    # выпуклой. Это плохо тем, что мы можем попасть в какой-то локальный минимум, который выше, чем глобальный.
    # Когда у нейронной сети или одного ее слоя все веса одинаковые, то создается проблема симметрии. Слой тогда
    # вырождается и ведет себя так же, как если бы он состоял из одного нейрона, хотя их в слое может быть несколько.

    return np.random.uniform(-0.5, 0.5, (l_out, l_in + 1))


def compute_cost_grad_approx(X, y, theta, lamb):
    # Примерно (численно) рассчитывает градиент функции стоимости. Он нужен для проверки алгоритма обратного
    # распространения ошибки. Сама функция считается достаточно долго.

    # Фукнция для численного расчета градиента фукници стоимости. Такой метод полезно использовать при проверке
    # правильности аналитического расчета.

    eps = 1e-5
    grad = np.zeros(theta.size)
    perturb = np.zeros(theta.size)
    for i in range(theta.shape[0]):
        if i % 1000 == 0:
            print(f'{round(i / theta.shape[0] * 100, 2)}%')
        perturb[i] = eps
        grad[i] = (compute_cost(X, y, theta + perturb, lamb) - compute_cost(X, y, theta - perturb, lamb)) / (2 * eps)
        perturb[i] = 0
    return grad


def logistic(z):
    # Функция принимает аргумент z - скаляр, вектор или матрицу в виде объекта numpy.array()
    # Должна возвращать скяляр, вектор или матрицу (в зависимости от размерности z)
    # результатов вычисления логистической функции от элементов z

    return 1 / (1 + np.exp(-z))


def logistic_grad(z):
    # Считает производную логистической функции.
    # f'(x) = f(x)(1 - f(x))
    # Функция принимает аргумент z - скаляр, вектор или матрицу в виде объекта numpy.array()
    # Должна возвращать скяляр, вектор или матрицу (в зависимости от размерности z)
    # результатов вычисления производной логистической функции от элементов z

    return logistic(z) * (1 - logistic(z))


def fictitous(X):
    # Функция добавления фиктивного признака (вектора) в матрицу

    return np.column_stack((np.ones(X.shape[0]), X))


def compute_hypothesis(X, theta):
    # X - матрица с m строками (столько примеров в обучающем наборе)
    # theta - вектор, поскольку матрица упакована в один большой вектор с помощью конкатенации (склеивания).
    # Это делается для удобства передачи матриц и оптимизации параметров, чтобы они были именно как вектор.

    # Функция принимает матрицу данных X без фиктивного признака и вектор параметров theta.
    # Вектор theta является "разворотом" мартиц Theta1 и Theta2 в плоский вектор.

    # ____________DIRECT DISTRIBUTION ALGORITHM EXPLAINED____________
    # Матрица связей между входным слоем и первым внутренним называется Theta1 - матрица,
    # задающая первый слой. У нее в строках находятся веса. Каждая строка соотвествует весам нейронов в 1-ом
    # скрытом слое. Иными словами, i-ая строка этой матрицы соотвествует вектору весов (параметров) i-го нейрона
    # 1-го скрытого слоя.
    # У этой матрицы будет S1 строк. S1 - количество нейронов в 1-ом скрытом слое.
    # Столбцов у нее будет столько, сколько есть входов + 1 элемент (фиктивный признак).
    # theta_0 - смещение нейрона.
    # Работа алгоритма:
    # Подали на вход некий вектор признаков. В нашем случае вектор признаков - пиксельные картинки (28x28). Мы его
    # так же конкатенируем (то есть из матрицы 28x28 делаем вектор из 784 элементов).
    # Далее нужно получить взвешенные входы нейронов скрытого слоя. Для этого матрицу Theta1 умножаем на веткор х, при
    # условии, что в нем уже учтен фиктивный признак. Получаем вектор z1 - входы нейронов 1-го скрытого слоя (то есть
    # взвешенные суммы входов).
    # Первую строку из Theta1 (соотвествует первому нейрону) умножаем на все входящие признаки и суммируем. В z1 будет
    # столько нейронов, сколько их в 1-ом скрытом слое. Все эти элементы являются числами и по мыслу обознают уже
    # взвешенный вход каждого нейрона 1-го скрытого слоя.
    # Выходы 1-го скрытого слоя обозначим за а1.
    # a1 = f(z1), где f - функция активации (логистическая)
    # Для полученных выходов мы повторяем предыдущую операцию.
    # z2 = Theta2 * a2
    # a3 = h_theta(x) = f(z2), где h_thete - выходы нейросети
    # Общая формула:
    # a_l+1 = f(Theta_l * a_l)
    # На каждый слой добавляется фиктивный признак.
    # _______________________________________________________________

    # Функция должна возвращать матрицу выходов сети для каждого примера из обучающего набора,
    # а также матрицу входов нейронов скрытого слоя и матрицу выходов скрытого слоя (это понадобится для реализации
    # алгоритма обратного распространения ошибки).

    # Восстановление матриц Theta1 и Theta2 из ветора theta:
    Theta1 = theta[:HIDDEN_LAYER_SIZE * (X.shape[1] + 1)].reshape(HIDDEN_LAYER_SIZE, X.shape[1] + 1)
    Theta2 = theta[HIDDEN_LAYER_SIZE * (X.shape[1] + 1):].reshape(10, HIDDEN_LAYER_SIZE + 1)

    Z2 = fictitous(X) @ Theta1.T  # Будущая матрица входов скрытого слоя, нужно заполнить
    A2 = logistic(Z2)  # Будущая матрица выходов скрытого слоя, нужно заполнить
    H = logistic(fictitous(A2) @ Theta2.T)  # Будущая матрица выходов, нужно заполнить

    # X - матрица-вектор строк, то есть примеры представлены вектор-строками, а ранее вектором-столбцами.
    # X * Theta (умножаем строку на строку), для этого Theta транспонируем
    # Z2 = X * Theta_T
    # A2 = f(Z2)
    # Z2 и A2 нужны для алгоритма обратного распространения.

    return H, Z2, A2


def compute_cost(X, y, theta, lamb):
    # Функция принимает матрицу данных X без фиктивного признака, вектор верных классов y,
    # вектор параметров theta и параметр регуляризации lamb.
    # Вектор theta является "разворотом" мартиц Theta1 и Theta2 в плоский вектор.
    # Должна вернуть значение функции стоимости в точке theta.

    m, n = X.shape  # m - количество примеров в выборке, n - количество признаков у каждого примера
    cost = 0  # значение функции стоимости при заданных параметрах, его нужно посчитать

    # Восстановление матриц Theta1 и Theta2 из ветора theta:
    # Theta1 = theta[:HIDDEN_LAYER_SIZE * (X.shape[1] + 1)].reshape(HIDDEN_LAYER_SIZE, X.shape[1] + 1)
    # Theta2 = theta[HIDDEN_LAYER_SIZE * (X.shape[1] + 1):].reshape(10, HIDDEN_LAYER_SIZE + 1)

    H, Z2, A2 = compute_hypothesis(X, theta)

    # По всем примерам из обучающей выборки
    for i in range(m):
        y_lbl = one_hot_encode(y[i])
        # По всем выходным нейронам, k - количество нейронов на выходном слое
        for k in range(H.shape[1]):
            cost += y_lbl[k] * np.log(H[i][k]) + (1 - y_lbl[k]) * np.log(1 - (H[i][k]))

    # Добавление регуляризации - суммирование квадратов абсолютно всех параметров нашей нейросети
    # (по всем слоям, по всем нейронам в слое и по всем нейронам в следующем слое)
    cost = -cost / m + lamb / (2 * m) * np.sum(theta ** 2)

    return cost


def compute_cost_grad(X, y, theta, lamb):
    # X - данные
    # y - метки данных
    # theta - все наши параметры
    # lamb - параметр регуляризации (так как мы строим нейросеть сразу с регуляризацией)

    # Функция вычисляет градиент функции стоимости по всем нашим параметрам, используя алгоритм обратного
    # распространения ошибки.

    # ____________ERROR BACK PROPAGATION ALGORITHM EXPLAINED____________
    # Это алгоритм вычисления производной некоторой сложной функции.
    # Нейросеть представляется в виде сложного вычислительного графа.
    # __________________________________________________________________

    # Мы используем алгоритм обратного распространения, который конкретно заточен под нашу функцию (с логистическими
    # функциями активации и прямого распространения). То есть для простых нейросетей, которые так же можно
    # назвать многослойными перцептронами.

    # Фиктивный признак мы не регуляризуем (j = 0)

    # Функция принимает матрицу данных X без фиктивного признака, вектор верных классов y,
    # вектор параметров theta и параметр регуляризации lamb.
    # Вектор theta является "разворотом" мартиц Theta1 и Theta2 в плоский вектор.
    # Функция должна реализовать алгоритм обратного распространения ошибки и вернуть матрицы градиентов весов связей.

    m, n = X.shape  # m - количество примеров в выборке, n - количество признаков у каждого примера

    # Восстановление матриц Theta1 и Theta2 из ветора theta:
    Theta1 = theta[:HIDDEN_LAYER_SIZE * (X.shape[1] + 1)].reshape(HIDDEN_LAYER_SIZE, X.shape[1] + 1)
    Theta2 = theta[HIDDEN_LAYER_SIZE * (X.shape[1] + 1):].reshape(10, HIDDEN_LAYER_SIZE + 1)

    Theta1_grad = None  # Будущая матрица градиентов весов между входным и скрытым слоем, ее нужно заполнить
    Theta2_grad = None  # Будущая матрица градиентов весов между скрытым и выходным слоем, ее нужно заполнить

    # Выполняем алгоритм прямого распространения - т. е. рассчитываем нашу гипотезу
    H, Z2, A2 = compute_hypothesis(X, theta)

    X = fictitous(X)
    A2 = fictitous(A2)
    Y = np.array([one_hot_encode(x) for x in y])

    # Считаем ошибку выходного слоя - выходы нейросети минус матрица из унитарного вектора
    delta3 = H - Y
    delta2 = []

    # Распространяем ошибку обратно, сначала матричное, потом поэлементное умножение
    for i in range(delta3.shape[0]):
        temp = (Theta2.T @ delta3[i]) * logistic_grad(fictitous(Z2)[i])
        delta2.append(temp)

    delta2 = np.array(delta2)

    delta2_new = []

    for i in range(delta2.shape[0]):
        delta2_new.append(delta2[i][1:])

    # Окончательные градиенты с регуляризацией
    delta1_big = (X.T @ delta2_new).T + lamb * Theta1
    delta2_big = (A2.T @ delta3).T + lamb * Theta2

    # Убираем регуляризацию фиктивного признака
    for i in range(delta1_big.shape[0]):
        delta1_big[i][0] = delta1_big[i][0] - lamb * Theta1[i][0]

    for i in range(delta2_big.shape[0]):
        delta2_big[i][0] = delta2_big[i][0] - lamb * Theta2[i][0]

    # Усредняем
    Theta1_grad = delta1_big / m
    Theta2_grad = delta2_big / m

    return np.concatenate((Theta1_grad, Theta2_grad), axis=None)


def load_data_idx(file_path):
    with open(file_path, 'rb') as input_file:
        magic = input_file.read(4)
        dims = int(magic[3])
        sizes = [struct.unpack('>L', input_file.read(4))[0] for _ in range(dims)]
        size = reduce(lambda x, y: x * y, sizes)
        data = np.array(list(input_file.read(size)), dtype=float)
        data = data.reshape(sizes)
        return data


# Загрузка данных
images = load_data_idx('images.idx')
features = images.reshape((images.shape[0], images.shape[1] * images.shape[2])) / 128 - 1.0

# Метки - правильные ответы для каждого изображения
labels = load_data_idx('labels.idx')

# Обучающие выборки
X = features[:SAMPLES_TO_TRAIN]
y = labels[:SAMPLES_TO_TRAIN]

# Тестовые выборки
X_test = features[SAMPLES_TO_TRAIN: SAMPLES_TO_TRAIN + SAMPLES_TO_TEST]
y_test = labels[SAMPLES_TO_TRAIN: SAMPLES_TO_TRAIN + SAMPLES_TO_TEST]

print(f'logistic(0) = {logistic(np.array(0))} (должно быть 0.5)\n'
      f'logistic(-10) = {logistic(np.array(-10))} (должно быть ~0)\n'
      f'logistic(10) = {logistic(np.array(10))} (должно быть ~1)')
print()
print(f'logistic_grad(0) = {logistic_grad(np.array(0))} (должно быть ~0.25)\n'
      f'logistic_grad(-10) = {logistic_grad(np.array(-10))} (должно быть ~0)\n'
      f'logistic_grad(10) = {logistic_grad(np.array(10))} (должно быть ~0)')

im_size = int(features.shape[1] ** 0.5)
hm = np.zeros((im_size * 10, im_size * 10))
for i in range(100):
    im_x = i % 10 * im_size
    im_y = i // 10 * im_size
    for j in range(features.shape[1]):
        px_x = im_x + j % im_size
        px_y = im_y + j // im_size
        hm[10 * im_size - 1 - px_x, px_y] = images[i, j % im_size, j // im_size]
plt.pcolor(hm, cmap='Greys')
plt.show()

lamb = 0.7  # можно поварьировать параметр регуляризации и посмотреть, что будет

init_Theta1 = np.random.uniform(-0.12, 0.12, (HIDDEN_LAYER_SIZE, X.shape[1] + 1))
init_Theta2 = np.random.uniform(-0.12, 0.12, (10, HIDDEN_LAYER_SIZE + 1))

init_theta = np.concatenate((init_Theta1, init_Theta2), axis=None)

back_prop_grad = compute_cost_grad(X[:10], y[:10], init_theta, 0)

print('Запуск численного расчета градиента (может занять какое-то время)')
num_grad = compute_cost_grad_approx(X[:10], y[:10], init_theta, 0)

print('Градиент, посчитанный аналитически: ')
print(back_prop_grad)

print('Градиент, посчитанный численно (должен примерно совпадать с предыдущим): ')
print(num_grad)

print()
print('Относительное отклоение градиентов (должно быть < 1e-7): ')
print((np.linalg.norm(back_prop_grad - num_grad) / np.linalg.norm(back_prop_grad + num_grad)))

print()
print('Запуск оптимизации параметров сети (может занять какое-то время)')
opt_theta_obj = minimize(lambda th: compute_cost(X, y, th, lamb), init_theta,
                         method='CG',
                         jac=lambda th: compute_cost_grad(X, y, th, lamb),
                         options={'gtol': 1e-3, 'maxiter': 3000, 'disp': False})

print('Минимизация функции стоимости ' + ('прошла успешно.' if opt_theta_obj.success else 'не удалась.'))

opt_theta = opt_theta_obj.x
print('Функция стоимости при оптимальных параметрах (должна быть < 0.5): ')
print(compute_cost(X, y, opt_theta, lamb))

hypos_train = compute_hypothesis(X, opt_theta)[0]

true_preds_train_num = 0
for k in range(hypos_train.shape[0]):
    max_el = np.max(hypos_train[k, :])
    true_preds_train_num += list(np.array(hypos_train[k, :] == max_el, dtype=np.int32)) == list(one_hot_encode(y[k]))
print('Доля верно распознаных цифр в обучающем наборе:', true_preds_train_num / hypos_train.shape[0] * 100, '%')

hypos_test = compute_hypothesis(X_test, opt_theta)[0]

true_preds_test_num = 0
for k in range(hypos_test.shape[0]):
    max_el = np.max(hypos_test[k, :])
    true_preds_test_num += list(np.array(hypos_test[k, :] == max_el, dtype=np.int32)) == list(one_hot_encode(y_test[k]))
print('Доля верно распознаных цифр в тестовом наборе:', true_preds_test_num / hypos_test.shape[0] * 100, '%')

# Выводим график того, как наша нейросеть думает.
# Это визуализация весов скрытого слоя (25 штук). Каждая картинка соотвествует весам этих нейронов. Каждый пиксель
# - это веса
# Свертка размера 28x28 - она меньше, чем входные данные. Там где белое - то идет в нейрон с плюсом, а там, где
# черное - с минусом. Серого много из-за регуляризации - можно попробовать покрутить этот параметр (lamb).
# Число нейронов с крытом слое должен быть квадратным числом (25, 36, 49 и тд)

if HIDDEN_LAYER_SIZE ** 0.5 % 1.0 == 0:
    Theta1 = opt_theta[:HIDDEN_LAYER_SIZE * (X.shape[1] + 1)].reshape(HIDDEN_LAYER_SIZE, X.shape[1] + 1)
    Theta1 = Theta1[:, 1:]
    Theta1 = (Theta1 - Theta1.mean(axis=0)) / (Theta1.std(axis=0))
    Theta1 = Theta1.reshape(HIDDEN_LAYER_SIZE, 28, 28)
    im_size = int(features.shape[1] ** 0.5)
    size_imgs = int(HIDDEN_LAYER_SIZE ** 0.5)
    hm = np.zeros((im_size * size_imgs, im_size * size_imgs))
    for i in range(HIDDEN_LAYER_SIZE):
        im_x = i % size_imgs * im_size
        im_y = i // size_imgs * im_size
        for j in range(features.shape[1]):
            px_x = im_x + j % im_size
            px_y = im_y + j // im_size
            hm[size_imgs * im_size - 1 - px_x, px_y] = Theta1[i, j % im_size, j // im_size]
    plt.pcolor(hm, cmap='Greys')
    plt.show()