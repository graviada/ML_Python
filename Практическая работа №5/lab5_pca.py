from PIL import Image
from matplotlib import pyplot as plt
from itertools import product
import random
import numpy as np


# Суть задачи сокращения размерности – максимально сократить множество признаков с заданными потерями информации. Это
# тоже задача категории обучения без учителя.
# Дано признаковое пространство - каждый объект описывается некоторым количеством признаков. Мы хотим сократь это
# количество признаков.
# Для решения этой задачи также существует множество методов, мы рассмотрим один из классических – метод главных
# компонент (МГК, PCA - principal component analysis).

# ------------------------------------------------------СУТЬ МГК--------------------------------------------------------
# Строятся новые признаки, как линейные комбинации имеющихся. Под признаком понимается некий вектор в признаковом
# пространстве. Эти векторы - орты. Координаты (если 3 признака): (1, 0, 0); (0, 1, 0); (0, 0, 1). Мы можем ввести
# другие признаки, не сонаправленные с нашими осями координат. Линейная комбинация - берем вектора, умножаем их на
# какие-то const и все это складываем.
# Далее мы переходим в новое признаковое пространство. Мы строим новые признаки из имеющихся, переходим в новый базис.
# В новом признаковом пространстве отбрасываются некоторые признаки.
# Новые признаки выбираются так, чтобы можно было отбросить некоторые из них с минимальными потерями информации.
# Под информацией понимается дисперсия вдоль некоторого вектора в признаковом пространстве (признака). Чем больше
# дисперсия, тем более информативен признак. Найденный вектор будет первой главной компонентой. Для трехмерного
# пространства получим 3 признака (вектора) с градацией дисперсией от большей к меньшей.
# ----------------------------------------------------------------------------------------------------------------------

# ----------------------------------------------------АЛГОРИТМ МГК------------------------------------------------------
# 1. Центрирование признаков.
# Вычитаем средние из всех наших признаков. i - индекс примера из об. выборки, j - номер его признака.
# 2. Расчет матрицы ковариации признаков.
# Есть N случайных величин. Матрица ковариации имеет размер NxN и описывает корреляцию линейную между i-ым и j-ым
# признаком из N.
# 3. Находим собственные вектора (u_1,…,u_n) и собственные значения (λ_1,…,λ_n) полученной матрицы. Лямбды - дисперсии
# вдоль соотвествующих векторов u.
# 4. Выбираем K (1 <= K <= n) векторов из полученного набора собственных векторов с наибольшими собственными значениями.
# Отсортировать вектора u по лямбда в порядке убывания и взять k первых векторов - они и будут искомыми главными
# компонентами.
# 5. Переводим имеющиеся данные из исходного признакового пространства в новое, базис которого состоит из главных
# компонент (лекция 49:30). Обратные к ортогональным матрицы являются просто транспонированными. u до k, а в векторе х n
# компонент. Сами вектора u_1,..,u_k содержат n элементов.
# Чтобы перевести объекты из нового пространства в старое, нужно выполнить обратное преобразование. При таком
# преобразовании произойдет потеря информации, но она минимальна.
# ----------------------------------------------------------------------------------------------------------------------

# От количества главных компонент зависит количество потерянной информации. Чем больше главных компонент оставим
# (признаков), тем меньше информации потеряется.
# Для ее оценки пользуются соотношением L. На практике K выбирают таким, чтобы L было не меньше 0.7 – 0.8.

# Советы:
# 1. Масштаб признаков влияет на дисперсию.
# 2. МГК работает лучше с недискретными количественными признаками (с вещественными переменными работает лучше, при этом
# понятие информации ближе к дисперсии).
# 3. МГК работает лучше, если между признаками есть линейные зависимости или близкие к ним (возникает потому, что мы
# используем ковариацию, то есть линейный показатель корреляции; метод позволяет убрать линейные зависимости между
# признаками и тем самым сократить размерность, потеряв наименьшее количество информации). Если это не так, лучше
# использовать другие методы (например, автокодировщики или анаморфозы).
# 4. МГК удобно использовать, если требуется визуализировать многомерное пространство признаков.
# 5. Не стоит применять МГК для борьбы с переобучением. Такой подход может сработать, но лучше использовать
# регуляризацию и другие методы.

# В данной части лабораторной работы предлагается реализовать метод главных компонент для сокращения
# размерности даннных. В качестве данных предлагаются изображения с лицами людей.

# Для выполнения этой работы необходимо заполнить код в следующих фукнциях:
# 1. pca - реализует алгоритм метода главных компонент.
# 2. project_data - проецирует данные в пространство с базисом, заданным матрицей перехода.
# 3. recover_data - восстанавливает спроецированные данные.


def load_data(data_file_path):
    # Функция для загрузки данных.
    # Принимает на вход путь к файлу.
    # Возвращает матрицу данных.

    with open(data_file_path) as input_file:
        X = list()
        for line in input_file:
            row = list(map(float, line.split(',')))
            X.append(row)
        return np.array(X, float)


def pca(X, k):
    # Функция, реализующая алгоритм метода главных компонент. Принимает на вход матрицу данных X и
    # число главных компонент k, которые мы хотим оставить.
    # Должна возвращать k главных компонент в виде матрицы (главные компоненты - столбцы) и долю сохраненной дисперсии
    # (сумма лямбд, деленных на сумму всех собственных чисел).

    m, n = X.shape  # m - количество примеров в матрице X, n - количество признаков.

    for j in range(n):
        X[:, j] = X[:, j] - np.mean(X[:, j])

    covariance_matrix = np.cov(X.T, bias=False)

    # size = covariance_matrix.shape()

    eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)

    indexes = eigen_values.argsort()[::-1]
    eigen_values_sorted = eigen_values[indexes]
    eigen_vectors_sorted = eigen_vectors[:, indexes]

    k_eigen_values = eigen_values_sorted[:k]
    k_eigen_vectors = eigen_vectors_sorted[:, :k]

    # матрица главных компонент
    U = k_eigen_vectors
    # доля сохранненой дисперсии после преобразования, от 0 до 1
    saved_disp = np.sum(k_eigen_values) / np.sum(eigen_values)

    return U, saved_disp


def project_data(X, U):
    # Функция для проекции данных в пространство с базисом, заданным матрицей U.
    # Принимает матрицу данных X (которую мы хотим спроецировать в пространство с базисом U) и матрицу U, задающую базис
    # пространства.
    # Должна возвращать новую матрицу данных, где все точки спроецированны в заданное пространство.
    # Мы из старого базиса переходим в новый. Для этого ищем обратную матрицу к U (в данном случае она
    # транспонированная).

    Xp = X @ U

    return Xp


def recover_data(Xp, U):
    # Из нового признакового пространства переходим в старое.
    # Функция для проекции данных из пространства с базисом, заданным матрицей U, в базис ijk.
    # Принимает матрицу данных Xp и матрицу U, задающую базис пространства.
    # Должна возвращать новую матрицу данных, где все точки спроецированны из заданного пространства,
    # в пространство с базисом ijk.

    X_rec = Xp @ U.T

    return X_rec


# Каждую картинку вытягиваем в вектор 32х32 в исходном признаковом пространстве. Каждое значение пикселя - это признак.
# И мы хотим сократить количество этих признаков.

# Стремные картинки - картинки, которые соотвествуют векторам - главным компонентам (первые 49). Дисперсии расположены
# по столбцам. Линейными комбинациями этих ихображений можно получить нормальное лицо.

X = load_data('faces.txt')

hm = np.zeros((32 * 7, 32 * 7))
for i in range(49):
    im_x = i % 7 * 32
    im_y = i // 7 * 32
    for j in range(X.shape[1]):
        px_x = im_x + j % 32
        px_y = im_y + j // 32
        hm[7 * 32 - 1 - px_x, px_y] = -X[i, j]
plt.pcolor(hm, cmap='Greys')
plt.show()

K = 1024  # количество главных компонент, можно поварьировать

U, disp = pca(X, K)

print('Доля сохраненной дисперсии (должна быть ~0.911 при K = 64):', disp)

Xp = project_data(X, U)
X_rec = recover_data(Xp, U)

hm = np.zeros((32 * 7, 32 * 7))
for i in range(49):
    im_x = i % 7 * 32
    im_y = i // 7 * 32
    for j in range(X.shape[1]):
        px_x = im_x + j % 32
        px_y = im_y + j // 32
        hm[7 * 32 - 1 - px_x, px_y] = -X_rec[i, j]
plt.pcolor(hm, cmap='Greys')
plt.show()

hm = np.zeros((32 * 7, 32 * 7))
for i in range(49):
    im_x = i % 7 * 32
    im_y = i // 7 * 32
    for j in range(X.shape[1]):
        px_x = im_x + j % 32
        px_y = im_y + j // 32
        hm[7 * 32 - 1 - px_x, px_y] = U.transpose()[i, j]
plt.pcolor(hm, cmap='Greys')
plt.show()
